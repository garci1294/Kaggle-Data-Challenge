{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "#Classifiers\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Model Validations\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FEATURE SELECTION #\n",
    "\n",
    "Input Parameter:\n",
    "(1) train_data: Training samples\n",
    "\n",
    "Output:\n",
    "(1) selected_columns: Features to choose for the training and testing samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corr_feature_selection(train_data):\n",
    "    #REf: https://towardsdatascience.com/feature-selection-correlation-and-p-value-da8921bfb3cf\n",
    "    \n",
    "    data_corr = train_data.corr()\n",
    "    \n",
    "    columns = np.full((data_corr.shape[0],), True, dtype=bool)\n",
    "    for i in range(data_corr.shape[0]):\n",
    "        for j in range(i+1, data_corr.shape[0]):\n",
    "            if data_corr.iloc[i,j] >= 0.9:\n",
    "                if columns[j]:\n",
    "                    columns[j] = False\n",
    "                   \n",
    "    selected_columns = train_data.columns[columns]\n",
    "    return selected_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFIER #\n",
    "\n",
    "Input Parameters:\n",
    "(1) xtrain = Training samples;\n",
    "(2) ytrain = Training Labels;\n",
    "(3) xtest = Testing samples;\n",
    "(4 - For KNN only) k = number of k neighbors to choose;\n",
    "(4 - For RF only) n_trees = no. of decision trees;\n",
    "(5) num_k_fold = k for the k-fold cross validation\n",
    "\n",
    "Output:\n",
    "(1) scores = five fold cross-validation scores;\n",
    "(2) rf_probs = prediction probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_implement(xtrain, ytrain, xtest, k, num_k_fold):\n",
    "    # implement a KNN classifier where k is based on the sample size; k = sqrt(sample_size)\n",
    "    \n",
    "    clf_knn = KNeighborsClassifier(n_neighbors=k)\n",
    "\n",
    "    # run five fold cross-validation\n",
    "    scores = cross_val_score(clf_knn, xtrain, ytrain, cv=num_k_fold, scoring='roc_auc')\n",
    "    \n",
    "    clf_knn.fit(xtrain, ytrain)\n",
    "    rf_probs = clf_knn.predict_proba(xtest)\n",
    "    \n",
    "    return scores, rf_probs\n",
    "\n",
    "def rf_implement(xtrain, ytrain, xtest, n_trees, num_k_fold):\n",
    "    # Implement Random Forest; get the five fold cross-validation\n",
    "    clf_RF = RandomForestClassifier(n_estimators=n_trees, \n",
    "                               bootstrap = True,\n",
    "                               max_features = 'sqrt')\n",
    "    \n",
    "    # run five fold cross-validation\n",
    "    scores = cross_val_score(clf_RF, xtrain, ytrain, cv=num_k_fold, scoring='roc_auc')\n",
    "    \n",
    "    # Fit on training data\n",
    "    clf_RF.fit(X, train_label)\n",
    "    results_RF = clf_RF.predict_proba(xtest)\n",
    "    \n",
    "    return scores, results_RF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OTHER FUNCTIONS #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_rt_mat(xnum, ynum, results):\n",
    "# Used the same code from the notebook provided in the competition\n",
    "# transfrom the results to prediction\n",
    "# clf.predict_proba returns a tuple with (probability_0, probability_1)\n",
    "\n",
    "    mat = np.zeros(shape=(xnum, ynum))\n",
    "    for ix, i in enumerate(results):\n",
    "        for jx, j in enumerate(results[ix]):\n",
    "            mat[jx, ix] = j[1]\n",
    "            \n",
    "    return mat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Code #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### Load datasets #####\n",
    "\n",
    "#Training data\n",
    "train_data = pd.read_csv(\"X_train.csv\", index_col=0)\n",
    "train_label = pd.read_csv(\"y_train.csv\", index_col=0)\n",
    "\n",
    "#Testing data\n",
    "X_test = pd.read_csv(\"X_test.csv\", index_col=0)\n",
    "\n",
    "# Data Preprocessing \n",
    "# Standarization training and testing data sets\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(train_data)\n",
    "X_test = scaler.fit_transform(X_test) \n",
    "\n",
    "#total training sample and features\n",
    "train_samp = train_data.shape[0]\n",
    "train_feat = train_label.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = round(math.sqrt(new_samp_count)) \n",
    "#print(\"The k for K-Nearest-neighbor: \", k)\n",
    "k_fold = 5\n",
    "trees = 100\n",
    "\n",
    "# implement a KNN classifier\n",
    "scores_knn, results_knn = knn_implement(X_train, train_label, X_test, k, k_fold)\n",
    "print(scores_knn)\n",
    "\n",
    "# Implement Random Forest model with 100 trees\n",
    "scores_RF, results_RF = rf_implement(X_train, train_label, X_test, trees, k_fold)\n",
    "print(scores_RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xnum = X_test.shape[0]\n",
    "ynum = 200\n",
    "\n",
    "mat_knn = convert_rt_mat(xnum, ynum, results_knn)\n",
    "mat_RF = convert_rt_mat(xnum, ynum, results_RF)\n",
    "\n",
    "mat = (mat_knn + mat_RF)/2 \n",
    "\n",
    "#Obtained from the Kaggle notebook.\n",
    "# Extract the sample and class names from the test submission\n",
    "y_test_sample = pd.read_csv(\"y_test_sample.csv\", index_col=0)\n",
    "\n",
    "# build the dataframe with proper index and column names\n",
    "df_results = pd.DataFrame(data=mat, index=y_test_sample.index, columns=y_test_sample.columns)\n",
    "\n",
    "# save to a file for submission\n",
    "df_results.to_csv(\"ind_knn_results.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FUTURE WORK RELATED #"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_data_distribution(train_label):\n",
    "#For each GO-term (ie. train label/class), identify samples that fall under that class and that do not.\n",
    "# Visulize the unbalanced datasets\n",
    "\n",
    "    label1_count = train_label.sum()\n",
    "    sample_size = train_label.shape[0]\n",
    "    label0_count = sample_size - label1_count\n",
    "\n",
    "    label1_count = label1_count.tolist()\n",
    "    label0_count = label0_count.tolist()\n",
    "\n",
    "    class_count.plot(kind='bar', title='Count (target)');\n",
    "    num_list = list(range(99))\n",
    "\n",
    "    plt.plot(num_list, label1_count[1:100], label = 'Train Label = 1')\n",
    "    plt.plot(num_list, label0_count[1:100], label = 'Train Label = 0')\n",
    "    plt.xlabel(\"Gene Annotation (Class)\")\n",
    "    plt.ylabel(\"Number of training sample for each GO class\")\n",
    "    plt.title('Unbalanced Training Dataset')\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance_train_data(y_train, GO_term, total_samp):\n",
    "# balancing data for classifier\n",
    "# Identify the lowest sample count (samples that are either 0 or 1)\n",
    "# and the balance the other one with randomly selecting the exact count\n",
    "\n",
    "    total_1 = sum(y_train[GO_term] == 1)\n",
    "    \n",
    "    total_0 = total_samp - total_1\n",
    "    new_samp_count = total_1 * 2\n",
    "\n",
    "    bal_train_label = pd.DataFrame()\n",
    "    if total_1 > total_0:\n",
    "\n",
    "        train_label_0 = train_label[GO_term][y_train[GO_term] == 0]\n",
    "        #find data associated to particular terms\n",
    "        curr_samp = pd.DataFrame()\n",
    "        curr_samp = train_label[GO_term][y_train[GO_term] == 1]\n",
    "        curr_samp = curr_samp.sample(n = total_0)\n",
    "        bal_train_label = pd.concat([train_label_0, curr_samp])\n",
    "\n",
    "    else:\n",
    "        train_label_1 = train_label[GO_term][y_train[GO_term] == 1]\n",
    "        #find data associated to particular terms\n",
    "        curr_samp = pd.DataFrame()\n",
    "        curr_samp = train_label[GO_term][y_train[GO_term] == 0]\n",
    "        curr_samp = curr_samp.sample(n = total_1)\n",
    "        bal_train_label = pd.concat([curr_samp, train_label_1])\n",
    "        \n",
    "    return bal_train_label, new_samp_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ind_knn_implement(xtrain, ytrain, xtest):\n",
    "    # store prediction results\n",
    "    mat = np.zeros(shape=(xtest.shape[0], 200))\n",
    "    #all five fold cross-validation for individual class\n",
    "    all_scores = []\n",
    "    #total training sample and features\n",
    "    train_samp = ytrain.shape[0]\n",
    "\n",
    "    for go_ix, GO_term in enumerate(all_GO_terms):\n",
    "        \n",
    "        bal_train_label, new_samp_count = balance_train_data(ytrain, GO_term, train_samp)\n",
    "\n",
    "        #Identify the training data to use\n",
    "        selected_data = bal_train_label.index.values\n",
    "        xtrain = train_data.loc[selected_data]\n",
    "        k = round(math.sqrt(new_samp_count))   \n",
    "\n",
    "        scores, results = knn_implement(xtrain, bal_train_label, xtest, k, 5)\n",
    "        all_scores.append(scores)\n",
    "\n",
    "        for ix, i in enumerate(results):\n",
    "            mat[ix, go_ix] = i[1] \n",
    "\n",
    "    return all_scores, mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_scores, mat = ind_knn_implement(X_train, train_label, X_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
